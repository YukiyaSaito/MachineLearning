{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([0,1,2])\n",
    "y = np.array([3,4,5])\n",
    "z = np.array([1,2,-1])\n",
    "A = np.array([[3,2,2],[1,3,1],[1,1,3]])\n",
    "alpha = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1: 14\n",
      "1.1.2: 0\n",
      "1.1.3: [2 6 2]\n",
      "1.1.4: 2.2360679775\n",
      "1.1.5: [6 5 7]\n",
      "1.1.6: 19\n",
      "1.1.7: [[11 10 10]\n",
      " [10 14 10]\n",
      " [10 10 14]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.inner(x,y) #1.1.1\n",
    "xz = np.inner(x,z) #1.1.2\n",
    "addition = alpha*(x+z) #1.1.3\n",
    "ipan = np.dot(x,np.transpose(z))+np.linalg.norm(x) #1.1.4\n",
    "matvec = np.dot(A,np.transpose(x))#1.1.5\n",
    "quad = np.inner(x,np.transpose(np.dot(A,np.transpose(x)))) #1.1.6\n",
    "mattrml=np.dot(np.transpose(A),A)\n",
    "\n",
    "print(\"1.1.1:\",xy)\n",
    "print(\"1.1.2:\",xz)\n",
    "print(\"1.1.3:\",addition)\n",
    "print(\"1.1.4:\",ipan)\n",
    "print(\"1.1.5:\",matvec)\n",
    "print(\"1.1.6:\",quad)\n",
    "print(\"1.1.7:\",mattrml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. True\n",
    "2. True\n",
    "3. False\n",
    "4. False\n",
    "5. False\n",
    "6. True\n",
    "7. False\n",
    "8. True\n",
    "9. True\n",
    "10. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1\n",
    "win: $\\frac{12}{36}$, lose: $\\frac{24}{36}$\n",
    "\n",
    "so, $15\\times\\frac{12}{36}+0\\times\\frac{24}{36} = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2\n",
    "\n",
    "$\\mathrm{Pr}(B)=0.55$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3\n",
    "Pr(B) = 0.916666 = 0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01009600000000001\n"
     ]
    }
   ],
   "source": [
    "pTDI=0.97\n",
    "pTDbI=1.0-0.99\n",
    "pDI=0.0001\n",
    "\n",
    "pTI=pDI*pTDI+(1.0-pDI)*pTDbI\n",
    "print(pTI) #2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3\n",
    "Use Bayes' theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009607765451664016\n"
     ]
    }
   ],
   "source": [
    "pDTI=(pDI*pTDI)/pTI\n",
    "print(pDTI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4\n",
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5\n",
    "Reduce the false positive probability (Improve the accuracy of negative results).\n",
    "\n",
    "If false positive rate is reduced by a factor of 10 (1% to 0.1%), the probability $P(D=1|T=1)$ becomes 8.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1\n",
    "$f'(x) = 6x -2$\n",
    "### 3.1.2\n",
    "$f'(x) = 1-2x$\n",
    "### 3.1.3\n",
    "$f'(x) = p(x)$\n",
    "\n",
    "$1-p(-x) = 1-\\dfrac{1}{1+e^{x}}\\quad=\\dfrac{1+e^x -1}{1+e^x} = \\dfrac{e^x}{1+e^x} = \\dfrac{1}{1+e^{-x}}=p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1\n",
    "\\begin{equation}\n",
    "\\nabla f(x) = \n",
    "\\begin{pmatrix}\n",
    "2x_1 + \\exp{(x_1 + 2x_2)}\\\\\n",
    "2\\exp(x_1 +2x_2) \n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### 3.2.2\n",
    "\\begin{equation}\n",
    "\\nabla f(x) = \\frac{1}{Z}\n",
    "\\begin{pmatrix}\n",
    "\\exp(x_1)\\\\\n",
    "\\exp(x_2)\\\\\n",
    "\\exp(x_3)\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### 3.2.3\n",
    "\\begin{equation}\n",
    "\\nabla f(x) = a\n",
    "\\end{equation}\n",
    "\n",
    "### 3.2.4\n",
    "\\begin{equation}\n",
    "\\nabla f(x) =\n",
    "\\begin{pmatrix}\n",
    "2x_1 -x_2\\\\\n",
    "2x_2 - x_1\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### 3.2.5\n",
    "\\begin{equation}\n",
    "\\nabla f(x) = x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1\n",
    "min: $\\dfrac{3}{14}$\n",
    "\n",
    "### 3.3.2\n",
    "max: $\\dfrac{1}{4}$\n",
    "\n",
    "### 3.3.3\n",
    "min: $0$\n",
    "\n",
    "### 3.3.4\n",
    "arg max: $x=\\dfrac{1}{2}$\n",
    "\n",
    "### 3.3.5\n",
    "min: $1$\n",
    "\n",
    "### 3.3.6\n",
    "arg min: $(x_1 , x_2)=(0,0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4\n",
    "[main.py (3.4)](main.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "### 4.1.1\n",
    "Minimum depth: 6\n",
    "\n",
    "### 4.1.2\n",
    "Minimum depth: 6\n",
    "\n",
    "\n",
    "## 4.2\n",
    "### 4.2.1 \n",
    "$O(n)$\n",
    "\n",
    "### 4.2.2\n",
    "$O(\\log{n})$ [(reference)](https://www.cs.ubc.ca/%7Eschmidtm/Courses/Notes/bigO.pdf)\n",
    "\n",
    "### 4.2.3\n",
    "Best case $O(1)$, worst case $O(n)$.  [(reference)](https://en.wikibooks.org/wiki/Data_Structures/Hash_Tables)\n",
    "\n",
    "### 4.2.4\n",
    "$O(d)$\n",
    "\n",
    "### 4.2.5\n",
    "$O(d^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3\n",
    "Define $T(f(N))$ as a runtime for a function $f(N)$  \n",
    "$T($ func1$(N)$ $)$ = $O(N)$  \n",
    "$T($ func2$(N)$ $)$ = $O(N)$  \n",
    "$T($ func3$(N)$ $)$ = $O(1)$  \n",
    "$T($ func4$(N)$ $)$ = $O(N^2)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "### 5.1.1\n",
    "Min:0.352, Max:4.862, Mean:1.325, Median:1.159, Mode:0.770\n",
    "\n",
    "### 5.1.2\n",
    "Quantile 5%:0.465, 25%:0.718, 50%:1.159, 75%:1.813, 95%:2.624\n",
    "\n",
    "### 5.1.3 \n",
    "Region with highest mean: WtdILI  \n",
    "Region with lowest mean: Pac  \n",
    "Region with highest variance: Mtn  \n",
    "Region with lowest variance: Pac  \n",
    "\n",
    "\n",
    "Draw a histogram with reasonably sized bins and possibly fit the distribution. Then the highest point of the histogram is a meaningflu \"mode\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2\n",
    "\n",
    "D - 1. A single histogram showing the distribution of each column in X.  \n",
    "    Reason: Because the histograms are plotted for each column in X. \n",
    "    \n",
    "C - 2. A histogram showing the distribution of each the values in the matrix X.  \n",
    "    Reason: Because it is showing one distribution of some values.\n",
    "\n",
    "B - 3. A boxplot grouping data by weeks, showing the distribution across regions for each week.      \n",
    "    Reason: B is the only one that has boxplot.\n",
    "\n",
    "A - 4. A plot showing the illness percentages over time.  \n",
    "    Reason: Illness percentage is shown in A and B, and B has already picked above.\n",
    "\n",
    "F - 5. A scatterplot between the two regions with highest correlation.  \n",
    "    Reason: Because the points seem to be on a $y=x$ line.\n",
    "\n",
    "E - 6. A scatterplot between the two regions with lowest correlation.  \n",
    "    Reason: Because the points are not so on a $y=x$ line.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "## 6.1\n",
    "When the feature is essentially discrete such as category, it would make sense to use equality-based rules.\n",
    "Example: Classify \"Human\" from \"Dog\", \"Cat\", and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2\n",
    "Decision Stump with inequality rule error (DecisionStumpErrorRate): 0.253  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3\n",
    "Decision Stump with inequality rule error (DecisionStumpInfoGain): 0.325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4\n",
    "Code saved in the /code directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5\n",
    "Error of Approach 1 (DecisionStumpErrorRate) is initially the smallest, however, it does not become any smaller after the depth of 6. On the other hand, other two approaches initially have larger errors than Approach 1 but they become smaller until depth of $\\sim10$.  \n",
    "\n",
    "Approach 1 (DecisionStumpErrorRate) seeks for a split which makes the error of the prediction is minimized, by looking at the mode of the groups made by the split. This is because initially it can achieve a relatively small error. However, in the long run, splits made that way are not globally optimized.  \n",
    "\n",
    "On the other hand, Approach 2 and 3 uses a cost function described by entropy, and the purpose of the function is to minimize the global entropy after splits are made. Therefore, the more depth the tree has, the more optimized the cost function becomes.  \n",
    "\n",
    "The difference between Approach 2 and 3 is that Approach 3 uses a non-deterministic method and this prevents the cost function from being stuck in local minimum, therefore it can achive smaller errors at larger depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6\n",
    "Even if the curves of classification error rate vs. tree depth are the same, it does not necessarily mean that those implimentations are the same. This is because there might be multiple local minima of the cost functions which could result in the same errors.\n",
    "In order to build confidence that the two implementations are the same, you should use those implementations for different datasets and see if they return the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7\n",
    "$O(mnd\\log n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/q6_decisionBoundary_ErrorRate.pdf\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
